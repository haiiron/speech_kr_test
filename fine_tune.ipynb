{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install torch\n",
    "! pip install torchaudio\n",
    "#! pip install tf-keras"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bc7d921907d270"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import\\\n",
    "    torchaudio\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, TrainingArguments, Trainer, AdamW, get_scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.243061Z",
     "start_time": "2025-04-01T16:09:13.318698Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# ----- Params -----\n",
    "\n",
    "# ë¼ë²¨ ë””ë ‰í„°ë¦¬\n",
    "# data_dir = \"datasets/OldPeople_Voice/label/\"\n",
    "data_dir = \"E:\\\\139-1.ì¤‘Â·ë…¸ë…„ì¸µ í•œêµ­ì–´ ë°©ì–¸ ë°ì´í„° (ê°•ì›ë„, ê²½ìƒë„)\\\\01-1.ì •ì‹ê°œë°©ë°ì´í„°\\\\Training\\\\02.ë¼ë²¨ë§ë°ì´í„°\\\\\"\n",
    "# ì˜¤ë””ì˜¤ ë””ë ‰í„°ë¦¬\n",
    "audio_dir = \"E:\\\\139-1.ì¤‘Â·ë…¸ë…„ì¸µ í•œêµ­ì–´ ë°©ì–¸ ë°ì´í„° (ê°•ì›ë„, ê²½ìƒë„)\\\\01-1.ì •ì‹ê°œë°©ë°ì´í„°\\\\Training\\\\01.ì›ì²œë°ì´í„°\\\\\"\n",
    "# í•™ìŠµëœ ë°ì´í„°\n",
    "save_dir = \"whisper_finetuned\"\n",
    "\n",
    "# Validation label data\n",
    "validation_label = \"E:\\\\139-1.ì¤‘Â·ë…¸ë…„ì¸µ í•œêµ­ì–´ ë°©ì–¸ ë°ì´í„° (ê°•ì›ë„, ê²½ìƒë„)\\\\01-1.ì •ì‹ê°œë°©ë°ì´í„°\\\\\"\n",
    "# Validation audio data\n",
    "validation_audio = \"E:\\\\139-1.ì¤‘Â·ë…¸ë…„ì¸µ í•œêµ­ì–´ ë°©ì–¸ ë°ì´í„° (ê°•ì›ë„, ê²½ìƒë„)\\\\01-1.ì •ì‹ê°œë°©ë°ì´í„°\\\\\"\n",
    "\n",
    "# ----- ------ -----"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.258210Z",
     "start_time": "2025-04-01T16:09:20.244062Z"
    }
   },
   "id": "8fe0e23fa401af65"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, json_list, processor):\n",
    "        self.processor = processor\n",
    "        self.data = []\n",
    "\n",
    "        # ëª¨ë“  JSON íŒŒì¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ\n",
    "        for json_path in json_list:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ\n",
    "            audio_file = os.path.join(audio_dir, data[\"fileName\"]+\".wav\")\n",
    "            \n",
    "            # íŒŒì¼ì´ ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì˜¤ë¥˜ ë°©ì§€)\n",
    "            if not os.path.exists(audio_file):\n",
    "                print(f\"âš ï¸ Warning: {audio_file} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "                continue  # í•´ë‹¹ íŒŒì¼ ê±´ë„ˆë›°ê¸°\n",
    "            \n",
    "            text = data[\"transcription\"][\"standard\"]\n",
    "            self.data.append((audio_file, text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, text = self.data[idx]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        # 16kHz ìƒ˜í”Œë§ for Whisper\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "        # ì˜¤ë””ì˜¤ ë°ì´í„° ë³€í™˜\n",
    "        input_features = self.processor(\n",
    "            waveform.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ í† í°í™” í•˜ê¸°\n",
    "        labels = self.processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features.squeeze(0),\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.273344Z",
     "start_time": "2025-04-01T16:09:20.259210Z"
    }
   },
   "id": "bf2efaeef5ad26af"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    audio_file = os.path.join(audio_dir, data[\"fileName\"]+\".wav\")\n",
    "    text = data[\"transcription\"][\"standard\"]\n",
    "\n",
    "    return {\n",
    "        \"audio\": audio_file,  # íŒŒì¼ ê²½ë¡œ ì €ì¥\n",
    "        \"text\": text,\n",
    "        # \"duration\": duration\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.289391Z",
     "start_time": "2025-04-01T16:09:20.274348Z"
    }
   },
   "id": "ee9aa5a117860435"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 1\n",
    "gradient_accumulation_steps = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:22.679579Z",
     "start_time": "2025-04-01T16:09:22.649581Z"
    }
   },
   "id": "97b3e65d9e001913"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ìˆ˜ 303148\n"
     ]
    }
   ],
   "source": [
    "json_list = glob(f\"{data_dir}**/*.json\", recursive=True)\n",
    "# print(sorted(json_list)[:10])\n",
    "print(\"data ìˆ˜\",len(json_list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:25.720769Z",
     "start_time": "2025-04-01T16:09:23.742407Z"
    }
   },
   "id": "caf4ffef4a757685",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json íŒŒì¼ ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ë\n"
     ]
    }
   ],
   "source": [
    "# JSON file ê²€ì‚¬\n",
    "# ë¼ë²¨ ë°ì´í„°ì— ë¬¸ì œ ë°œìƒ ì‹œ... ì§ì ‘ ìˆ˜ì • ë°”ëë‹ˆë‹¤.\n",
    "# 139-1.ì¤‘Â·ë…¸ë…„ì¸µ í•œêµ­ì–´ ë°©ì–¸ ë°ì´í„° (ê°•ì›ë„, ê²½ìƒë„)\\01-1.ì •ì‹ê°œë°©ë°ì´í„°\\Training\\02.ë¼ë²¨ë§ë°ì´í„°\\TL_02. ê²½ìƒë„_01. 1ì¸ë°œí™” ë”°ë¼ë§í•˜ê¸°\\st_set1_collectorgs100_speakergs442_54_10 ì—ì„œ , ì¤‘ë³µ ë¬¸ì œ ìˆì—ˆìŒ!\n",
    "json_path = \"\"\n",
    "try:\n",
    "    for json_path in json_list:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json.load(f)\n",
    "    print(\"json íŒŒì¼ ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ë\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(json_path,\", label data ì˜¤ë¥˜ë°œìƒ!!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:12:32.336900Z",
     "start_time": "2025-04-01T16:09:26.360998Z"
    }
   },
   "id": "cb1fa11e93361b95",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperEncoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n      (embed_positions): WhisperPositionalEmbedding(448, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperDecoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"SungBeom/whisper-small-ko\")\n",
    "processor = WhisperProcessor.from_pretrained(\"SungBeom/whisper-small-ko\")\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:12:34.914846Z",
     "start_time": "2025-04-01T16:12:32.337892Z"
    }
   },
   "id": "17fc863a270ddbc1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303148ê°œ ë¡œë“œ.\n",
      "steps ì´ 75787\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomAudioDataset(json_list, processor)\n",
    "print(f\"{len(dataset)}ê°œ ë¡œë“œ.\")\n",
    "train_dataloader = DataLoader(dataset,\n",
    "                              num_workers=12, # CPU ë³‘ë ¬í™”\n",
    "                              pin_memory=True, # GPU ì²˜ë¦¬ ê°€ì†\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda x: x\n",
    "                              )\n",
    "print(f\"steps ì´ {len(train_dataloader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:17:47.114049Z",
     "start_time": "2025-04-01T16:14:21.664157Z"
    }
   },
   "id": "7ca657b529865494"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haiiron\\.conda\\envs\\cuda_38\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer & scheduler \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "\n",
    "print(\"device : \",next(model.parameters()).device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:17:47.129701Z",
     "start_time": "2025-04-01T16:17:47.115076Z"
    }
   },
   "id": "252abca12ecef155"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model couunt loop.  1\n"
     ]
    }
   ],
   "source": [
    "error_count = 0  # ì—ëŸ¬ ë°œìƒ íšŸìˆ˜ ì €ì¥\n",
    "count_loop = 0\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    count_loop+= 1\n",
    "    print(\"train model couunt loop. \", count_loop)\n",
    "    model.train() # í•™ìŠµ ëª¨ë“œë¡œ\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        try:\n",
    "            print(\" ğŸš€ try ğŸš€ \", step)\n",
    "            # ë°°ì¹˜ì—ì„œ input_featuresì™€ labels ì¶”ì¶œ\n",
    "            input_features = [item[\"input_features\"].to(device) for item in batch]\n",
    "            labels = [item[\"labels\"].to(device) for item in batch]\n",
    "\n",
    "            # padding ì²˜ë¦¬\n",
    "            input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True, padding_value=0)\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "            # ëª¨ë¸ì— ì…ë ¥\n",
    "            outputs = model(input_features, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps  # gradient accumulation ì ìš©\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1 == len(train_dataloader)):\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"ï¸ Error at step {step}: {e}\")\n",
    "            continue  # ì—ëŸ¬ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê¸°\n",
    "\n",
    "    avg_loss = total_loss / (len(train_dataloader) - error_count)\n",
    "    print(f\"ğŸš€ Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Errors: {error_count}\")\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    model.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n",
    "    processor.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-04-01T16:17:47.130731Z"
    }
   },
   "id": "30f84c857290e6fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b06d938581bc1970"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model_path = \"whisper_finetuned/epoch_1\"  # XëŠ” ì €ì¥í•œ ì—í¬í¬ ë²ˆí˜¸\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ íŒŒì¼\n",
    "audio_file = audio_dir + \"ë…¸ì¸ë‚¨ì—¬_ë…¸ì¸ëŒ€í™”77_F_ê¹€XX_62_ì œì£¼_ì‹¤ë‚´_84051.WAV\"\n",
    "\n",
    "# ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "# WhisperëŠ” 16kHz ìƒ˜í”Œë§ ì†ë„ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³€í™˜ í•„ìš”\n",
    "if sample_rate != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "# ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜\n",
    "input_features = processor(waveform.squeeze(0).numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "# ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "# ì˜ˆì¸¡ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
    "transcribed_text = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"ì˜ˆì¸¡ëœ í…ìŠ¤íŠ¸:\", transcribed_text)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c049e24048e1b03c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
