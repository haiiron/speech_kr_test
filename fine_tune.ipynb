{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install torch\n",
    "! pip install torchaudio\n",
    "#! pip install tf-keras"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bc7d921907d270"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import\\\n",
    "    torchaudio\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, TrainingArguments, Trainer, AdamW, get_scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.243061Z",
     "start_time": "2025-04-01T16:09:13.318698Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# ----- Params -----\n",
    "\n",
    "# 라벨 디렉터리\n",
    "# data_dir = \"datasets/OldPeople_Voice/label/\"\n",
    "data_dir = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\Training\\\\02.라벨링데이터\\\\\"\n",
    "# 오디오 디렉터리\n",
    "audio_dir = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\Training\\\\01.원천데이터\\\\\"\n",
    "# 학습된 데이터\n",
    "save_dir = \"whisper_finetuned\"\n",
    "\n",
    "# Validation label data\n",
    "validation_label = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\\"\n",
    "# Validation audio data\n",
    "validation_audio = \"E:\\\\139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\\\01-1.정식개방데이터\\\\\"\n",
    "\n",
    "# ----- ------ -----"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.258210Z",
     "start_time": "2025-04-01T16:09:20.244062Z"
    }
   },
   "id": "8fe0e23fa401af65"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, json_list, processor):\n",
    "        self.processor = processor\n",
    "        self.data = []\n",
    "\n",
    "        # 모든 JSON 파일을 리스트로\n",
    "        for json_path in json_list:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # 오디오 파일 경로\n",
    "            audio_file = os.path.join(audio_dir, data[\"fileName\"]+\".wav\")\n",
    "            \n",
    "            # 파일이 실제 존재하는지 확인 (오류 방지)\n",
    "            if not os.path.exists(audio_file):\n",
    "                print(f\"⚠️ Warning: {audio_file} 파일이 존재하지 않습니다.\")\n",
    "                continue  # 해당 파일 건너뛰기\n",
    "            \n",
    "            text = data[\"transcription\"][\"standard\"]\n",
    "            self.data.append((audio_file, text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, text = self.data[idx]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        # 16kHz 샘플링 for Whisper\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "        # 오디오 데이터 변환\n",
    "        input_features = self.processor(\n",
    "            waveform.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "\n",
    "        # 텍스트 토큰화 하기\n",
    "        labels = self.processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features.squeeze(0),\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.273344Z",
     "start_time": "2025-04-01T16:09:20.259210Z"
    }
   },
   "id": "bf2efaeef5ad26af"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    audio_file = os.path.join(audio_dir, data[\"fileName\"]+\".wav\")\n",
    "    text = data[\"transcription\"][\"standard\"]\n",
    "\n",
    "    return {\n",
    "        \"audio\": audio_file,  # 파일 경로 저장\n",
    "        \"text\": text,\n",
    "        # \"duration\": duration\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:20.289391Z",
     "start_time": "2025-04-01T16:09:20.274348Z"
    }
   },
   "id": "ee9aa5a117860435"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 1\n",
    "gradient_accumulation_steps = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:22.679579Z",
     "start_time": "2025-04-01T16:09:22.649581Z"
    }
   },
   "id": "97b3e65d9e001913"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 수 303148\n"
     ]
    }
   ],
   "source": [
    "json_list = glob(f\"{data_dir}**/*.json\", recursive=True)\n",
    "# print(sorted(json_list)[:10])\n",
    "print(\"data 수\",len(json_list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:09:25.720769Z",
     "start_time": "2025-04-01T16:09:23.742407Z"
    }
   },
   "id": "caf4ffef4a757685",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json 파일 데이터 무결성 검사 끝\n"
     ]
    }
   ],
   "source": [
    "# JSON file 검사\n",
    "# 라벨 데이터에 문제 발생 시... 직접 수정 바랍니다.\n",
    "# 139-1.중·노년층 한국어 방언 데이터 (강원도, 경상도)\\01-1.정식개방데이터\\Training\\02.라벨링데이터\\TL_02. 경상도_01. 1인발화 따라말하기\\st_set1_collectorgs100_speakergs442_54_10 에서 , 중복 문제 있었음!\n",
    "json_path = \"\"\n",
    "try:\n",
    "    for json_path in json_list:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json.load(f)\n",
    "    print(\"json 파일 데이터 무결성 검사 끝\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(json_path,\", label data 오류발생!!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:12:32.336900Z",
     "start_time": "2025-04-01T16:09:26.360998Z"
    }
   },
   "id": "cb1fa11e93361b95",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperEncoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n      (embed_positions): WhisperPositionalEmbedding(448, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperDecoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"SungBeom/whisper-small-ko\")\n",
    "processor = WhisperProcessor.from_pretrained(\"SungBeom/whisper-small-ko\")\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:12:34.914846Z",
     "start_time": "2025-04-01T16:12:32.337892Z"
    }
   },
   "id": "17fc863a270ddbc1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303148개 로드.\n",
      "steps 총 75787\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomAudioDataset(json_list, processor)\n",
    "print(f\"{len(dataset)}개 로드.\")\n",
    "train_dataloader = DataLoader(dataset,\n",
    "                              num_workers=12, # CPU 병렬화\n",
    "                              pin_memory=True, # GPU 처리 가속\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda x: x\n",
    "                              )\n",
    "print(f\"steps 총 {len(train_dataloader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:17:47.114049Z",
     "start_time": "2025-04-01T16:14:21.664157Z"
    }
   },
   "id": "7ca657b529865494"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haiiron\\.conda\\envs\\cuda_38\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer & scheduler \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "\n",
    "print(\"device : \",next(model.parameters()).device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T16:17:47.129701Z",
     "start_time": "2025-04-01T16:17:47.115076Z"
    }
   },
   "id": "252abca12ecef155"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model couunt loop.  1\n"
     ]
    }
   ],
   "source": [
    "error_count = 0  # 에러 발생 횟수 저장\n",
    "count_loop = 0\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    count_loop+= 1\n",
    "    print(\"train model couunt loop. \", count_loop)\n",
    "    model.train() # 학습 모드로\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        try:\n",
    "            print(\" 🚀 try 🚀 \", step)\n",
    "            # 배치에서 input_features와 labels 추출\n",
    "            input_features = [item[\"input_features\"].to(device) for item in batch]\n",
    "            labels = [item[\"labels\"].to(device) for item in batch]\n",
    "\n",
    "            # padding 처리\n",
    "            input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True, padding_value=0)\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "            # 모델에 입력\n",
    "            outputs = model(input_features, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps  # gradient accumulation 적용\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1 == len(train_dataloader)):\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"️ Error at step {step}: {e}\")\n",
    "            continue  # 에러 발생 시 건너뛰기\n",
    "\n",
    "    avg_loss = total_loss / (len(train_dataloader) - error_count)\n",
    "    print(f\"🚀 Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Errors: {error_count}\")\n",
    "\n",
    "    # 모델 저장\n",
    "    model.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n",
    "    processor.save_pretrained(f\"{save_dir}/epoch_{epoch+1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-04-01T16:17:47.130731Z"
    }
   },
   "id": "30f84c857290e6fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b06d938581bc1970"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model_path = \"whisper_finetuned/epoch_1\"  # X는 저장한 에포크 번호\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 테스트 오디오 파일\n",
    "audio_file = audio_dir + \"노인남여_노인대화77_F_김XX_62_제주_실내_84051.WAV\"\n",
    "\n",
    "# 오디오 파일 로드\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "# Whisper는 16kHz 샘플링 속도를 사용하므로 변환 필요\n",
    "if sample_rate != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
    "\n",
    "# 모델의 입력으로 변환\n",
    "input_features = processor(waveform.squeeze(0).numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "# 모델을 통해 예측 수행\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "# 예측된 텍스트 디코딩\n",
    "transcribed_text = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"예측된 텍스트:\", transcribed_text)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c049e24048e1b03c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
